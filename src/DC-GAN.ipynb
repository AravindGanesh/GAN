{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional GAN \n",
    "Reference : https://arxiv.org/abs/1511.06434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target \n",
    "DC GAN on celebA, cifar-10, cifar-100 datasets\n",
    "###### To report\n",
    "+ Train - parameters and architecture \n",
    "+ saved weights for 3 datasets \n",
    "+ loss, number of epochs\n",
    "+ generated images : modes vs quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plan\n",
    "\n",
    "+ read to tf.Dataset() format\n",
    "+ Intialize with parameters\n",
    "+ model class ()\n",
    "+ `noise(batch_size, dim, distribution='uniform'):`  returns `tf.data.Dataset()`\n",
    "+ def `generator(z):` returns image\n",
    "+ def `discriminator(img):` returns probability\n",
    "+ def `batch_norm` : returns normalized batch\n",
    "+ def `**LOSS**(real, fake):` returns losses of G and D\n",
    "+ def `save():` saves model to a file (probably `.h5` file)\n",
    "+ def `load(filename):` loads weights of the model\n",
    "+ def `generate_images(noise)`\n",
    "+ def `evaluate(x_test)`\n",
    "\n",
    "#### Architecture guidelines for stable Deep Convolutional GANs - reference: GAN paper\n",
    "+ Replace any pooling layers with strided convolutions (discriminator) and fractional-strided convolutions (generator).\n",
    "+ Use batchnorm in both the generator and the discriminator. _Do not apply batchnorm to G output layer and D input layer_\n",
    "+ Remove fully connected hidden layers for deeper architectures.\n",
    "+ Use ReLU activation in generator for all layers except for the output, which uses Tanh.\n",
    "+ Use LeakyReLU activation in the discriminator for all layers. In the LeakyReLU, the slope of the leak was set to 0.2 in all models.\n",
    "+ Scale images to range of $[-1,1]$\n",
    "+ minibatch stocastic gradient - batch_size = 128\n",
    "+ All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02.\n",
    "\n",
    "+ Adam optimizer: learning_rate = 0.0002, Î²1 = 0.5\n",
    "+ We found global average pooling increased model stability but hurt convergence speed. A middle ground of directly connecting the highest convolutional features to the input and output respectively of the generator and discriminator worked well.\n",
    "+ simple image de-duplication process. We fit a 3072-128-3072 de-noising dropout regularized RELU autoencoder on 32x32 downsampled center-crops of training examples.\n",
    "+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class DC_GAN():\n",
    "    def __init__(self, sess, real_img_dim, gen_img_dim, noise_dim, batch_size, num_channels=3):\n",
    "        self.sess = sess\n",
    "        self.noise_dim = noise_dim\n",
    "        self.D_input_dim = real_img_dim\n",
    "        self.G_output_dim = gen_img_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.channels = num_channels\n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def noise(self, batch_size, noise_dim, pdf='uniform'): \n",
    "        '''\n",
    "        Inputs: batch size while training, dimension of noise\n",
    "        returns random sample from the distribution - pdf (uniform by default)\n",
    "        '''\n",
    "        batch_size = self.batch_size\n",
    "        noise_dim = self.noise_dim\n",
    "        if pdf=='uniform':\n",
    "            return tf.random_uniform(minval=-1, maxval=1, shape=(batch_size, noise_dim), name='z')\n",
    "        if pdf=='normal':\n",
    "            return tf.random_normal(mean=0, stddev=1, shape=(batch_size, noise_dim), name='z')\n",
    "        \n",
    "    def batch_norm(self, batch):\n",
    "        return tf.contrib.layers.batch_norm(inputs=batch,\n",
    "            decay=0.9,\n",
    "            updates_collections=None,\n",
    "            epsilon=1e-5,\n",
    "            scale=True,\n",
    "            is_training=True\n",
    "            )\n",
    "    \n",
    "    def add_conv_layer(self, inputs, filter_size, in_channels, out_channels, scope_name):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            shape = [filter_size, filter_size, in_channels, out_channels]\n",
    "            weights = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "            biases = tf.Variable(tf.constant(0.05, shape=[out_channels]))\n",
    "            \n",
    "            layer = tf.nn.conv2d(\n",
    "                input=inputs,\n",
    "                filter=weights,\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME') + biases\n",
    "            \n",
    "            layer = tf.nn.leaky_relu(self.batch_norm(layer), alpha=0.2)\n",
    "            return layer, weights, biases\n",
    "            \n",
    "    def add_conv_transpose_layer(self,inputs,filter_size,in_channels,out_channels,scope_name,activation):\n",
    "        with tf.variable_scope(scope_name):\n",
    "            shape = [filter_size, filter_size, out_channels, in_channels]\n",
    "            weights = tf.Variable(tf.truncated_normal(shape, stddev=0.02))\n",
    "            biases = tf.Variable(tf.constant(0.05, shape=[out_channels]))\n",
    "\n",
    "            layer = tf.nn.conv2d_transpose(\n",
    "                input=inputs,\n",
    "                filter=weights,\n",
    "                strides=[1, 2, 2, 1],\n",
    "                padding='SAME') + biases\n",
    "\n",
    "            if activation == 'relu': layer = tf.nn.relu(self.batch_norm(layer))\n",
    "            elif activation == 'tanh': layer = tf.nn.tanh(self.batch_norm(layer))\n",
    "\n",
    "            return layer, weights, biases\n",
    "\n",
    "    def discriminator(self, img):\n",
    "        scope_name='discriminator'\n",
    "        h1, w1, b1 = self.add_conv_layer(\n",
    "            inputs = img,\n",
    "            filter_size=5,\n",
    "            in_channels=self.num_channels,\n",
    "            out_channels=64,\n",
    "            scope_name=scope_name,\n",
    "        )\n",
    "        h2, w2, b2 = self.add_conv_layer(\n",
    "            inputs=h1,\n",
    "            filter_size=5,\n",
    "            in_channels=64,\n",
    "            num_filters=128,\n",
    "            scope_name=scope_name,\n",
    "        )\n",
    "        h3, w3, b3 = self.add_conv_layer(\n",
    "            inputs=h2,\n",
    "            filter_size=5,\n",
    "            in_channels=128,\n",
    "            out_channels=256,\n",
    "            scope_name=scope_name,\n",
    "        )\n",
    "        h4, w4, b4 = self.add_conv_layer(\n",
    "            inputs=h3,\n",
    "            filter_size=5,\n",
    "            in_channels=256,\n",
    "            out_channels=512,\n",
    "            scope_name=scope_name,\n",
    "        )\n",
    "        logits = tf.layers.dense(\n",
    "            inputs=tf.reshape(h4, shape=[self.batch_size, -1]),\n",
    "            units=1,\n",
    "            use_bias=True,\n",
    "            kernel_initalizer=tf.contrib.layers.xavier_initializer()\n",
    "        )\n",
    "        return logits\n",
    "        \n",
    "    def generator(self, z):\n",
    "        scope_name = 'generator'\n",
    "        _z = tf.layers.dense(\n",
    "            inputs=z,\n",
    "            units=4*4*1024,\n",
    "            use_bias=True,\n",
    "            kernel_initalizer=tf.contrib.layers.xavier_initializer()\n",
    "        ) # projection of z to 4x4x1024 layer (linear)\n",
    "        h1 = tf.reshape(_z, [-1, 4,4,1024])\n",
    "        h2, w2, b2 = self.add_conv_transpose_layer(\n",
    "            inputs=h1,\n",
    "            filter_size=5,\n",
    "            in_channels=1024,\n",
    "            out_channels=512,\n",
    "            scope_name=scope_name,\n",
    "            activation='relu'\n",
    "        )\n",
    "        h3, w3, b3 = self.add_conv_transpose_layer(\n",
    "            inputs=h2,\n",
    "            filter_size=5,\n",
    "            in_channels=512,\n",
    "            out_channels=256,\n",
    "            scope_name=scope_name,\n",
    "            activation='relu'\n",
    "        )\n",
    "        h4, w4, b4 = self.add_conv_transpose_layer(\n",
    "            inputs=h4,\n",
    "            filter_size=5,\n",
    "            in_channels=256,\n",
    "            out_channels=128,\n",
    "            scope_name=scope_name,\n",
    "            activation='relu'\n",
    "        )\n",
    "        G_out, w5, b5 = self.add_conv_transpose_layer(\n",
    "            inputs=h4,\n",
    "            filter_size=5,\n",
    "            in_channels=128,\n",
    "            num_filters=self.num_channels,\n",
    "            scope_name=scope_name,\n",
    "            activation='tanh'\n",
    "        )\n",
    "        return G_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
